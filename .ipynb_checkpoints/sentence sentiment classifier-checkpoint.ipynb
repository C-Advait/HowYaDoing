{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "biological-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesser-cherry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large')\n",
    "model.train()\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "??model.classifier.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varied-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "??model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "breeding-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifer_output(self,features, **kwargs):\n",
    "    print('features shape is: ', features.shape)\n",
    "    x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "    print('x shape is: ', x.shape)\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.out_proj(x)\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(x.size(0))\n",
    "    return x\n",
    "model.classifier.forward = classifer_output.__get__(model.classifier)\n",
    "??model.classifier.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thermal-brand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape is:  torch.Size([3, 8, 1024])\n",
      "x shape is:  torch.Size([3, 1024])\n",
      "tensor([[ 0.1313,  0.7318],\n",
      "        [-0.0789,  0.7044],\n",
      "        [ 0.1481,  0.6587]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([3, 2])\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.5981, grad_fn=<NllLossBackward>), logits=tensor([[ 0.1313,  0.7318],\n",
       "        [-0.0789,  0.7044],\n",
       "        [ 0.1481,  0.6587]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing what inputs are taken and how model takes them\n",
    "inputs = tokenizer([\"Hello, my dog is cute\", \"Today is a wonderful day\", \"I hate my boss\"],\\\n",
    "                   padding = True, truncation = True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1, 1, 0]).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "# display(labels)\n",
    "# display(labels.shape)\n",
    "# display(inputs['input_ids'])\n",
    "# display(inputs['input_ids'].shape)\n",
    "\n",
    "outputs = model(input_ids = inputs['input_ids'], \n",
    "                attention_mask = inputs['attention_mask'],\n",
    "                labels=labels\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preceding-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tired-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset emotion (/home/advait/.cache/huggingface/datasets/emotion/default/0.0.0/aa34462255cd487d04be8387a2d572588f6ceee23f784f37365aa714afeb8fe6)\n"
     ]
    }
   ],
   "source": [
    "# load emotions dataset for fine-tuning model\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amino-private",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset)\n",
    "train = dataset[\"train\"]\n",
    "test = dataset[\"test\"]\n",
    "validation = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "effective-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize dataset\n",
    "train_encodings = tokenizer(train[\"text\"], truncation=True, padding=True)\n",
    "train.rename_column_(\"label\", \"labels\")\n",
    "\n",
    "validation_encodings = tokenizer(validation['text'], truncation = True, padding = True)\n",
    "validation.rename_column_('label', 'labels')\n",
    "\n",
    "display(train)\n",
    "display(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quantitative-capability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'labels': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "found-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_encodings['input_ids']:\n",
    "    if len(item) != 88:\n",
    "        print('length is: ', len(item))\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "optimum-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(\n",
    "    train_encodings, \n",
    "    train['labels'], \n",
    ")\n",
    "validation_dataset = EmotionDataset(\n",
    "    validation_encodings,\n",
    "    validation['labels'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exposed-danish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=6, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change number of output labels\n",
    "num_labels = 6\n",
    "in_features = 1024\n",
    "\n",
    "model.classifier = torch.nn.Linear(\n",
    "    in_features = in_features,\n",
    "    out_features = num_labels,\n",
    "    bias = True\n",
    ")\n",
    "model.num_labels = num_labels\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "behind-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "??model.classifier.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "backed-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_classifer_output(self,features, **kwargs):\n",
    "    x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "    x = F.linear(x, self.weight, self.bias)\n",
    "    return x\n",
    "\n",
    "model.classifier.forward = new_classifer_output.__get__(model.classifier)\n",
    "??model.classifier.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imperial-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "injured-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = transformers.AdamW(model.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "consistent-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_item = train_dataset.__getitem__(0)\n",
    "# display(test_item['input_ids'].shape)\n",
    "# display(test_item['attention_mask'].shape)\n",
    "# display(test_item[\"labels\"].unsqueeze(0).shape)\n",
    "# outputs = model(\n",
    "#     input_ids = test_item['input_ids'],\n",
    "#     attention_mask = test_item['attention_mask'],\n",
    "#     labels = test_item['labels'].unsqueeze(0)\n",
    "# )\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "visible-period",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 88])\n",
      "torch.Size([8, 88])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels'].unsqueeze(0)\n",
    "    print(input_ids.shape)\n",
    "    print(attention_mask.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "silver-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 1.666\n",
      "[1,  1000] loss: 1.618\n",
      "[1,  1500] loss: 1.607\n",
      "[1,  2000] loss: 1.595\n",
      "\n",
      " epoch 1 loss is:  1.621579302251339\n",
      "[2,   500] loss: 1.607\n",
      "[2,  1000] loss: 1.579\n",
      "[2,  1500] loss: 1.593\n",
      "[2,  2000] loss: 1.606\n",
      "\n",
      " epoch 2 loss is:  1.5961248015761376\n",
      "[3,   500] loss: 1.594\n",
      "[3,  1000] loss: 1.604\n",
      "[3,  1500] loss: 1.578\n",
      "[3,  2000] loss: 1.584\n",
      "\n",
      " epoch 3 loss is:  1.5902965607345103\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()\n",
    "model.to('cuda')\n",
    "running_loss = 0.0\n",
    "epoch_loss = 0.0\n",
    "for epoch in range(3):\n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        labels = batch['labels'].unsqueeze(0).to('cuda')\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        if i % 500 == 499:    # print every 500 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 500))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print(f\"\\n epoch {epoch+1} loss is: \", epoch_loss/len(train_loader))\n",
    "    epoch_loss = 0.0\n",
    "print('Finished Training')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "spiritual-pricing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 5625,   21,   10,  372,  183,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs = tokenizer([\"I am feeling very joyful today\", \"Today was a great day\", \"I love my life\"],\\\n",
    "                       padding = True, truncation = True, return_tensors = \"pt\")\n",
    "single_test = tokenizer([\"Today was a great day\"], return_tensors = \"pt\")\n",
    "single_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "statistical-recording",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5196,  0.6951, -0.8676, -0.0977, -0.0780, -1.4826]],\n",
       "       grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model(**single_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "thirty-anime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5617,  0.7354, -0.6782,  0.0535,  0.0914, -1.4523],\n",
       "        [ 0.5794,  0.8002, -0.8169, -0.0448,  0.1982, -1.4509],\n",
       "        [ 0.5542,  0.7254, -0.8334, -0.0685,  0.0226, -1.4838]],\n",
       "       grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "virtual-communist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5727,  0.6854, -0.8902, -0.0637, -0.0450, -1.4432]],\n",
       "       grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anger_input = tokenizer([\"I had a horrible day\"], return_tensors = \"pt\")\n",
    "outputs = model(**anger_input)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-hudson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
